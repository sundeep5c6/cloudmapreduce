This page gives a high level view of the source code structure. It is designed to get a new contributor started quickly. However, keep in mind that, there is no substitute to reading the definitive guide -- the source code. 

= Database interface =

Cloud !MapReduce uses a central "database" to store the job status. The intent is to have several interfaces, one for each "database" that we support, so that the users can choose the best based on the environment. We currently only support SimpleDB, but the functionalities are well isolated in a single file `com.acnlabs.CloudMapReduce.DbManager` that it should be fairly easy to port to other "databases". 

The code in this file is broken down into several well isolated code blocks. The related functions are group close to each other in the source code. 

== node synchronization == 

Nodes can update their status (either "running" or "complete") for each phase (either "setup", "map", "reduce") to the data store. This information is used to synchronize all nodes. Currently, it is only used for synchronizing at the "setup" stage. A few nodes (by default 1) are responsible for creating the reduce queues beforehand, no other nodes should proceed until the setup is finished. The map and reduce stages synchronize using a commit mechanism discussed below.

The following routines are used to update the status to the data store.

{{{
public void startTask()
public void completeTask()
private void updateStatus()
}}}

The following routines are used to read the collective states reported by all nodes and determine if a phase is finished. 

{{{
public boolean isPhaseComplete()
public void waitForPhaseComplete()
private int getPhaseSize()
}}}

== Reduce conflict resolution == 

In rare occasions, two nodes may process the same reduce job at the same time. Eventually, they will realize there is a conflict and will attempt to resolve conflict through the data store. 

They use the following routine to report a suspicion that there is a conflict. 

{{{
public void claimReduce()
}}}

Then, they use the following routine to query the list of potential parties involved in the conflict and resolve to a single owner of the reduce work. 

{{{
public String resolveReduce()
}}}

== Map and Reduce commit mechanism == 

We use a commit mechanism to guard against failures and to ensure all parts of the job is completed. Whenever a node completes a map or reduce task successfully, it commits the result to the data store by calling the following:

{{{
public void commitTask()
}}}

Towards the end of a map or reduce stage, nodes start to query the data store for all completed work by checking the commit messages. They use the following functions and class.

{{{
private synchronized void insertCommittedTaskWinners()
private class CollectCommittedTaskRunnable implements Runnable
public HashSet<String> getCommittedTask()
public Boolean isStageFinished()
}}}

The `Runnable` class is used to spawn several threads to query multiple SimpleDB domains in parallel. We use multiple domains to increase the overall write throughput. Beyond simply determining whether all map or reduce work has been completed, Cloud !MapReduce also uses `getCommittedTask` to get a list of valid results so that it can filter messages in the queues. 

== Determine size of reduce queues ==

We must keep track of how many messages are written to each reduce queue, so that we know how many to expect when we process it. After finishing a map task, the node updates to the data store the number of messages it generated for each reduce queue by calling the following. 

{{{
public void updateReduceOutputPerMap()
}}}

At the beginning of processing a reduce queue, the node queries the data store to see how many messages to expect. It sums up all values reported by all map nodes by calling the following functions and class. 

{{{
private synchronized void addReduceQSize()
private class CollectReduceQSizeRunnable implements Runnable
public int getReduceQSize()
}}}

Again, the `Runnable` is used to spawn multiple domains to increase the write throughput. 
 
== Queue interface ==

Similar to the database interface, the queue interface is intended to be portable, so that we can run on a different infrastructure than Amazon in the future. Even though today the pure queue processing and the SQS interface parts are mixed in the same file, it is relatively easy to separate out the two functions in the future to support a different queue implementation, such as [http://activemq.apache.org/ ActiveMQ].

`com.acnlabs.CloudMapReduce.QueueManager` contains two queue implementations, both conform to the interface defined by `com.acnlabs.CloudMapReduce.SimpleQueue` and `com.acnlabs.CloudMapReduce.SimpleQueueAdapter`. 

One queue implementation is called `SimpleQueueImpl`. It implements the standard Push and Pop interfaces and it interfaces with SQS directly. It contains a variable `maxNumMessages` (configurable by user) which indicates how many SQS messages it attempts to keep in its buffer for reading. If the number of messages falls below the threshold, it spawns many `PopRunnable` objects to download from SQS in parallel. For writing to SQS, it always spawns `PushRunnable` to upload the message to SQS in the background. `SimpleQueueImpl` tags messages before sending them in order to identify which node and which map task generated the messages, and it filters messages when receiving them in order to remove duplicate and redundant messages. 

The other queue implementation is called `EfficientQueue`, which is built on top of `SimpleQueueImpl`. It aggregates small messages into a big one before sending it to the underlying `SimpleQueueImpl`. Similarly, it de-multiplex small messages from a big one when reading from `SimpleQueueImpl`. 

`QueueManager` also contains several useful functions for creating SQS queues, listing SQS queues so that it knows what additional queue needs to be created, and deleting SQS queues for clean up. 

== File system interface == 

Cloud !MapReduce uses S3 as the file system. Like the database and queue interface, the file system interface could be replaced in the future to support other file systems, such as that provided by [http://project-voldemort.com/ Project Voldemort]. 

The file system interface is captured in two classes: `com.acnlabs.CloudMapReduce.S3FileSystem` and `com.acnlabs.CloudMapReduce.S3Item`. They are very short, so we do not elaborate further here. 

= Cloud !MapReduce applications =

There are three sample applications in the `com.acnlabs.CloudMapReduce.application` package: `Grep`, `ReverseIndex`, and `WordCount`. See [HowToWriteApp the how to write apps tutorial] for more understanding of the application code. 

Currently, all application code inherits from `MapReduceApp` class. It handles the common tasks such as command line parsing, setting up the environment to launch the job, creating and populating the input queues. 

Some more rethinking in this area is needed in terms of what needs to be extracted out for all applications, code refactoring, and how to support cascaded !MapReduce jobs. 

= !MapReduce interfaces =

There are several interfaces defined to ensure a consistent implementation. The `Mapper` and `Reducer` interfaces define how the user-defined map and reduce functions should be written. All Cloud !MapReduce jobs must support these interfaces.

In addition, `OutputCollector` defines the standard interface for an application to output the key/value pairs. The `MapCollector`, `CombineCollector` and `ReduceCollector` (defined in `com.acnlabs.CloudMapReduce.mapreduce.MapReduce`) all implements the same interface.

= Supporting classes =

== Thread pool ==

== Performance tracker ==